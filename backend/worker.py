from kafka import KafkaConsumer
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import json
import redis
import spacy

# –ó–∞–≥—Ä—É–∑–∫–∞ spaCy
nlp = spacy.load("ru_core_news_sm")

# Redis –∫–ª–∏–µ–Ω—Ç
redis_client = redis.Redis(host="localhost", port=6379, db=0)

# –ú–æ–¥–µ–ª—å
model_path = "./model"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path).to("cuda" if torch.cuda.is_available() else "cpu")

tokenizer.pad_token = tokenizer.eos_token
model.resize_token_embeddings(len(tokenizer))

# –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è
def detect_category(text: str) -> str:
    categories = {
    'IT': ['windows','–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ','–∞—Å—Å–µ–º–±–ª–µ—Ä','it','—Å–∏—Å–∞–¥–º–∏–Ω','1—Å','–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç', '–∞–π—Ç–∏', '–∫–æ–¥', '–∞–ª–≥–æ—Ä–∏—Ç–º', '—Å–µ—Ä–≤–µ—Ä', '–±–∞–≥', 'python', '—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫', '—Ç–µ—Å—Ç–µ—Ä', '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω', '–∫–æ–º–ø—å—é—Ç–µ—Ä', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç', '—Å–∞–π—Ç', '–¥–∏–∑–∞–π–Ω–µ—Ä', '–≥–µ–π–º–¥–µ–≤','–∏—Å–∫—É—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç','–Ω–µ–π—Ä–æ—Å–µ—Ç—å','–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ'],
    '–®–∫–æ–ª–∞': ['–ø–µ—Ä–µ–º–µ–Ω–∞','–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞','—É—á–∏—Ç–µ–ª—å', '—à–∫–æ–ª–∞', '–≤–æ–≤–æ—á–∫–∞', '–∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è', '—É—Ä–æ–∫', '–∫–ª–∞—Å—Å', '—É—á–µ–Ω–∏–∫', '–¥–Ω–µ–≤–Ω–∏–∫', '–¥–æ–º–∞—à–∫–∞', '—Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–µ —Å–æ–±—Ä–∞–Ω–∏–µ'],
    '–†–∞–±–æ—Ç–∞': ['–æ—Ç—á—ë—Ç','—Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ','—Å–µ–∫—Ä–µ—Ç–∞—Ä—à–∞','—Ä–∞–±–æ—Ç–∞', '–Ω–∞—á–∞–ª—å–Ω–∏–∫', '–æ—Ñ–∏—Å', '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫', '–∑–∞—Ä–ø–ª–∞—Ç–∞', '–æ—Ç–ø—É—Å–∫', '–∫–∞—Ä—å–µ—Ä–∞', '–æ—Ñ–∏—Å–Ω—ã–π –ø–ª–∞–Ω–∫—Ç–æ–Ω', '–±–æ—Å—Å', '–º–µ–Ω–µ–¥–∂–µ—Ä'],
    '–î–æ–∫—Ç–æ—Ä–∞': ['–≤—Ä–∞—á', '–¥–æ–∫—Ç–æ—Ä', '–º–µ–¥–∏–∫', '–±–æ–ª—å–Ω–∏—Ü–∞', '–ø–∞—Ü–∏–µ–Ω—Ç', '–∞–ø—Ç–µ—á–∫–∞', '–≤–µ—Ç–µ—Ä–∏–Ω–∞—Ä', '–ø–æ–ª–∏–∫–ª–∏–Ω–∏–∫–∞', '–æ–ø–µ—Ä–∞—Ü–∏—è', '–ø—Ä–∏—ë–º', '–ª–µ—á–µ–Ω–∏–µ','–ø—Ä–æ–∫—Ç–æ–ª–æ–≥'],
    '–°—Ç—É–¥–µ–Ω—Ç—ã': ['–ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä','—Å—Ç—É–¥–µ–Ω—Ç', '—Å–µ—Å—Å–∏', '–∑–∞—á—ë—Ç', '–æ–±—â–µ–∂–∏—Ç', '–ø—Ä–µ–ø–æ–¥', '–ª–µ–∫—Ü–∏', '–¥–∏–ø–ª–æ–º', '–∫—É—Ä—Å–æ–≤–∞', '–º–∞–≥–∏—Å—Ç—Ä', '–±–∞–∫–∞–ª–∞–≤—Ä'],
    '–°–µ–º—å—è': ['–º–∞–º–∞','–ø–∞–ø–∞','–∂–µ–Ω–∞', '–º—É–∂', '—Å–µ–º—å', '—Ç–µ—â', '—Å–≤—ë–∫—Ä', '–¥–µ—Ç–∏', '—Ä–µ–±—ë–Ω–æ–∫', '–±—Ä–∞–∫', '—Ä–æ–¥–∏—Ç–µ–ª–∏', '–≤–æ—Å–ø–∏—Ç–∞–Ω–∏–µ', '—Ä–∞–∑–≤–æ–¥','–æ—Ç–µ—Ü','–≤–Ω—É–∫','–¥–µ–¥','–±–∞–±–∫–∞','–±–∞–±—É—à–∫–∞'],
    '–ü–æ–ª–∏—Ç–∏–∫–∞': ['–º–µ–¥–≤–µ–¥–µ–≤','–±—Ä–µ–∂–Ω–µ–≤','—Å–≤–æ','–ø–æ–ª–∏—Ç–∏–∫', '–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç', '–¥–µ–ø—É—Ç–∞—Ç', '–≤—ã–±–æ—Ä', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤', '–∂–∏—Ä–∏–Ω–æ–≤—Å–∫–∏–π', '–ø—É—Ç–∏–Ω', '—Ç—Ä–∞–º–ø', '—Å–∞–Ω–∫—Ü–∏–∏', '—Ä–µ–≤–æ–ª—é—Ü–∏—è', '–º–∏—Ç–∏–Ω–≥', '–∫—Ä–µ–º–ª—å', '–±–∞–π–¥–µ–Ω','–∫–æ–º–º—É–Ω–∏—Å—Ç','—á–∏–Ω–æ–≤–Ω–∏–∫','–≤–æ–π–Ω–∞','–ª–µ–Ω–∏–Ω','—Å—Ç–∞–ª–∏–Ω','–º—ç—Ä'],
    '18+': ['–±–ª—è—Ç—å', '—Å—É–∫', '—Ö—É–π', '–µ–±–∞', '–ø–∏–∑–¥', '–Ω–∞—Ö—É–π', '—Ç—Ä–∞—Ö', '–º–∞–Ω–¥–∞', '–≥–∞–Ω–¥–æ–Ω', '—Å–µ–∫—Å', '—Ç—Ä–∞—Ö–∞–µ—Ç', '–µ–±–∞', '–ø–∏–¥–æ—Ä', '—ë–±', '–¥—Ä–æ—á', '–∂–æ–ø–∞', '—Å—Å–∞–Ω', '–≥–æ–≤–Ω', '–±–ª—è', '–ø–µ–∑–¥', '—à–ª—é—Ö'],
    '–ë–∞—Ä': ['–±–∞—Ä–º–µ–Ω', '–±–∞—Ä', '–ø–∏–≤–æ', '—É–ª–∏—Ç–∫–∞', '–≤–æ–¥–∫–∞', '–ø—å—è–Ω—ã–π', '–±–∞—Ä–Ω–∞—è —Å—Ç–æ–π–∫–∞', '–≤–∏—Å–∫–∏', '—à–æ—Ç'],
    '–ê—Ä–º–∏—è': ['—Å–æ–ª–¥–∞—Ç', '—Å–µ—Ä–∂–∞–Ω—Ç', '—Ä–æ—Ç', '–æ—Ñ–∏—Ü–µ—Ä', '–¥–µ–º–±–µ–ª—å', '–ø–æ—Ä—É—á–∏–∫', '–†–∂–µ–≤—Å–∫–∏–π', '–∞—Ä–º–∏—è', '–ø—Ä–∏—Å—è–≥–∞', '–≤–æ–µ–Ω–Ω—ã–π', '–∫–∞–∑–∞—Ä–º–∞'],
    '–°–ø–æ—Ä—Ç': ['–∫—É–±–æ–∫','–∞—Ä–µ–Ω–∞','–∫–µ—Ä–∂–∞–∫–æ–≤','–∑–µ–Ω–∏—Ç','—Å–ø–∞—Ä—Ç–∞–∫','–∞–±—Ä–∞–º–æ–≤–∏—á','—Ñ—É—Ç–±–æ–ª', '—Ö–æ–∫–∫–µ–π', '—Ç—Ä–µ–Ω–µ—Ä', '–º–∞—Ç—á', '–æ–ª–∏–º–ø–∏–∞–¥', '—Å–±–æ—Ä–Ω', '—Å–ø–æ—Ä—Ç—Å–º–µ–Ω', '—Ç—É—Ä–Ω–∏—Ä', '—á–µ–º–ø–∏–æ–Ω–∞—Ç', '–≥–æ–ª', '–ø–µ–Ω–∞–ª—å—Ç–∏'],
    '–®—Ç–∏—Ä–ª–∏—Ü': ['—à—Ç–∏—Ä–ª–∏—Ü', '–∫—ç—Ç', '–º—é–ª–ª–µ—Ä'],
    '–ù–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å': ['–∞–º–µ—Ä–∏–∫–∞–Ω–µ—Ü','—á—É–∫—á','—Ä—É—Å—Å–∫–∏–π', '–µ–≤—Ä–µ–π', '–Ω–µ–º–µ—Ü', '–Ω–µ–≥—Ä', '–∞—Ä–º—è–Ω–∏–Ω', '–≥—Ä—É–∑–∏–Ω', '—Ö–æ—Ö–æ–ª', '—Ö–æ—Ö–ª—ã', '—á–µ—á–µ–Ω–µ—Ü', '–¥–∞–≥–µ—Å—Ç–∞–Ω–µ—Ü', '—É–∫—Ä–∞–∏–Ω–µ—Ü', '–∫–∞–∑–∞—Ö'],
    '–î—Ä—É–≥–æ–µ': []
}
    lemmas = [t.lemma_ for t in nlp(text.lower())]
    for cat, words in categories.items():
        if any(w in lemmas for w in words):
            return cat
    return "–î—Ä—É–≥–æ–µ"

# Kafka consumer
consumer = KafkaConsumer(
    "joke_requests",
    bootstrap_servers="localhost:9092",
    value_deserializer=lambda m: json.loads(m.decode("utf-8")),
    auto_offset_reset="earliest",
    group_id="joke_worker"
)

print("‚úÖ Worker –∑–∞–ø—É—â–µ–Ω...")

for msg in consumer:
    data = msg.value
    task_id = data.get("task_id")
    if not task_id:
        print("‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ –∑–∞–¥–∞—á–∏ –±–µ–∑ ID")
        continue

    prompt_text = data.get("prompt", "").strip()
    print(f"üì• –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–¥–∞—á–∏: {task_id} | prompt={prompt_text[:30]}...")
    length = data.get("length", "–∫–æ—Ä–æ—Ç–∫–∏–π")
    obscene = data.get("obscene", False)

    category = detect_category(prompt_text) if prompt_text else data.get("category", "–î—Ä—É–≥–æ–µ")

    if prompt_text:
        prompt = f"<s>{prompt_text}"
    else:
        mat = "—Å –º–∞—Ç–æ–º" if obscene else "–±–µ–∑ –º–∞—Ç–∞"
        prompt = f"–†–∞—Å—Å–∫–∞–∂–∏ {length} –∞–Ω–µ–∫–¥–æ—Ç –ø—Ä–æ {category} {mat}:\n"

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    output = model.generate(
        **inputs,
        max_new_tokens=80,
        do_sample=True,
        temperature=0.9,
        top_k=50,
        top_p=0.95,
        repetition_penalty=1.2,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id
    )

    decoded = tokenizer.decode(output[0], skip_special_tokens=False)
    result = decoded.replace(prompt, "").split("</s>")[0].strip()

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º result –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø—Ä–∞–≤–∏–ª—å–Ω–æ!
    redis_client.setex(f"joke_result:{task_id}", 3600, json.dumps({
        "response": result,
        "category": category
    }))

    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ: task_id={task_id}, category={category}")